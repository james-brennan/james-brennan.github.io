<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Estimating CDFs from data done right - James Brennan</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta itemprop="name" content="Estimating CDFs from data done right">
<meta itemprop="description" content="This post looks at estimating empirical cumulative density functions (CDFs) and their confidence intervals from data.">
<meta itemprop="datePublished" content="2020-04-11T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2020-04-11T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="994">



<meta itemprop="keywords" content="statistics,probability,uncertainty,python,CDF," /><meta property="og:title" content="Estimating CDFs from data done right" />
<meta property="og:description" content="This post looks at estimating empirical cumulative density functions (CDFs) and their confidence intervals from data." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://james-brennan.github.io/posts/edf/" />
<meta property="article:published_time" content="2020-04-11T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-04-11T00:00:00+00:00" /><meta property="og:site_name" content="James Brennan" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Estimating CDFs from data done right"/>
<meta name="twitter:description" content="This post looks at estimating empirical cumulative density functions (CDFs) and their confidence intervals from data."/>
<link href='https://fonts.googleapis.com/css?family=Playfair+Display:700' rel='stylesheet' type='text/css'>
	<link rel="stylesheet" type="text/css" media="screen" href="https://james-brennan.github.io/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://james-brennan.github.io/css/main.css" />

	<script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
		<script src="https://james-brennan.github.io/js/main.js"></script>
</head>

<body>
	<div class="container wrapper">
		<div class="header">
	
		<div class="avatar">
			<a href="https://james-brennan.github.io/">
				<img src="/img/avatar.jpg" alt="James Brennan" />
			</a>
		</div>
	
	<h1 class="site-title"><a href="https://james-brennan.github.io/">James Brennan</a></h1>
	<div class="site-description"><p>A blog about scientific computing, statistics, geoscience and remote sensing.</p><nav class="nav social">
			<ul class="flat"><li><a href="https://github.com/james-brennan/" title="Github"><i data-feather="github"></i></a></li><li><a href="https://twitter.com/jamesbrennan/" title="Twitter"><i data-feather="twitter"></i></a></li><li><a href="/index.xml" title="RSS"><i data-feather="rss"></i></a></li></ul>
		</nav></div>

	<nav class="nav">
		<ul class="flat">
			
			<li>
				<a href="/">Home</a>
			</li>
			
			<li>
				<a href="/posts">All posts</a>
			</li>
			
			<li>
				<a href="/tags">Tags</a>
			</li>
			
		</ul>
	</nav>
</div>


		<div class="post">
			<div class="post-header">
				
					<div class="meta">
						<div class="date">
							<span class="day">11</span>
							<span class="rest">Apr 2020</span>
						</div>
					</div>
				
				<div class="matter">
					<h1 class="title">Estimating CDFs from data done right</h1>
				</div>
			</div>
					
			<div class="markdown">
				<p>This post looks at estimating empirical cumulative density functions (CDFs) and their confidence intervals from data.</p>
<p>Typically when we have some data an initial exploratory analysis is to consider how our data varies. Lots of the time we might have a good idea of the statistical distribution our data is likely drawn from, measurements collected by a satellite might follow a normal distribution. We could use our data to estimate the moments of the Normal distribution to get our population distribution. Sometimes though we do not have a good idea of whether our distribution follows a parametric distribution and want to turn to non-parametric methods. We do this by estimating the <a href="https://en.wikipedia.org/wiki/Empirical_distribution_function">empirical distribution function</a> (the empirical cumulative distribution function CDF) of our data.</p>
<p>To start, we&rsquo;ll sample some data from our (un)known population:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#00a">import</span> <span style="color:#0aa;text-decoration:underline">scipy.stats</span>
<span style="color:#00a">import</span> <span style="color:#0aa;text-decoration:underline">matplotlib.pyplot</span> <span style="color:#00a">as</span> <span style="color:#0aa;text-decoration:underline">plt</span>
<span style="color:#00a">import</span> <span style="color:#0aa;text-decoration:underline">seaborn</span> <span style="color:#00a">as</span> <span style="color:#0aa;text-decoration:underline">sns</span>
<span style="color:#00a">import</span> <span style="color:#0aa;text-decoration:underline">numpy</span> <span style="color:#00a">as</span> <span style="color:#0aa;text-decoration:underline">np</span>
<span style="color:#aaa;font-style:italic"># create our data</span>
data = scipy.stats.gamma(<span style="color:#099">5</span>,<span style="color:#099">1</span>).rvs(<span style="color:#099">200</span>)
</code></pre></div><p>And producing histograms (frequency counts) of the data is super easy of course:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">fig, ax = plt.subplots(<span style="color:#099">1</span>,<span style="color:#099">2</span>, figsize=(<span style="color:#099">8</span>, <span style="color:#099">3</span>))
ax[<span style="color:#099">0</span>].hist(data,lw=<span style="color:#099">1</span>,edgecolor=<span style="color:#a50"></span><span style="color:#a50">&#39;</span><span style="color:#a50">k</span><span style="color:#a50">&#39;</span>)
ax[<span style="color:#099">1</span>].hist(data,lw=<span style="color:#099">1</span>,edgecolor=<span style="color:#a50"></span><span style="color:#a50">&#39;</span><span style="color:#a50">k</span><span style="color:#a50">&#39;</span>, cumulative=True)
[a.set_xlabel(<span style="color:#a50"></span><span style="color:#a50">&#34;</span><span style="color:#a50">x</span><span style="color:#a50">&#34;</span>) <span style="color:#00a">for</span> a <span style="color:#00a">in</span> ax];
[a.set_ylabel(<span style="color:#a50"></span><span style="color:#a50">&#34;</span><span style="color:#a50">Frequency</span><span style="color:#a50">&#34;</span>) <span style="color:#00a">for</span> a <span style="color:#00a">in</span> ax];
sns.despine()
</code></pre></div><p><img src="/img//EDF_3_0.png" alt="png"></p>
<p>Where the figure on left is an ordinary histogram (i.e. the number of samples in each frequency bin) and the right is the cumulative count of samples for each bin: the cumulative histogram. Of course at the moment these are not strictly empirical distributions until they are normalised to provide discrete probability and cumulative distribution functions:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">fig, ax = plt.subplots(<span style="color:#099">1</span>,<span style="color:#099">2</span>, figsize=(<span style="color:#099">8</span>, <span style="color:#099">3</span>))
ax[<span style="color:#099">0</span>].hist(data, lw=<span style="color:#099">1</span>,edgecolor=<span style="color:#a50"></span><span style="color:#a50">&#39;</span><span style="color:#a50">k</span><span style="color:#a50">&#39;</span>, density=True)
ax[<span style="color:#099">1</span>].hist(data,lw=<span style="color:#099">1</span>,edgecolor=<span style="color:#a50"></span><span style="color:#a50">&#39;</span><span style="color:#a50">k</span><span style="color:#a50">&#39;</span>, cumulative=True, density=True)
[a.set_xlabel(<span style="color:#a50"></span><span style="color:#a50">&#34;</span><span style="color:#a50">$x$</span><span style="color:#a50">&#34;</span>) <span style="color:#00a">for</span> a <span style="color:#00a">in</span> ax];
[a.set_ylabel(<span style="color:#a50"></span><span style="color:#a50">&#34;</span><span style="color:#a50">Density</span><span style="color:#a50">&#34;</span>) <span style="color:#00a">for</span> a <span style="color:#00a">in</span> ax];
sns.despine()
</code></pre></div><p><img src="/img//EDF_4_0.png" alt="png"></p>
<h2 id="the-empirical-distribution-function">The empirical distribution function</h2>
<p>Thinking more about how the cumulative density is produced provides a route for not just relying on matplotlib and provides us with a deeper understanding of the quality of our distribution. The empirical distribution function of our data $\hat{F}(x)$ is the cumulative distribution function (CDF) that cumulatively increases by $1/n$ at each data point $X_i$:</p>
<p>$$ \hat{F}(x)  = \frac{\sum_{i=1}^{N} \text{I}(X_i \leq x)}{n}$$</p>
<p>where $\text{I}$ provides an indicator function that essentially provides a mathematical function to count the number of samples below $x$:</p>
<p>$$ \text{I}(X_i \leq x) = \begin{cases}
1 &amp; \text{if} &amp; X_i \leq x \\\<br>
0 &amp; \text{if} &amp; X_i &gt; x
\end{cases} $$</p>
<p>$X_i$ is less than or equal to the current value of $x$, $\text{I}(X_i)$ equals 1. So at each increment along $x$ we count the number of observations $[X_i \dots X_N]$ that are below or equal to $x$.</p>
<p>Let&rsquo;s implement this in python and see how it works:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#00a">def</span> <span style="color:#0a0">edf</span>(data):
    x0 = data.min() <span style="color:#aaa;font-style:italic"># get sample space</span>
    x1 = data.max()
    x = np.linspace(x0, x1, <span style="color:#099">100</span>) 
    N = data.size <span style="color:#aaa;font-style:italic"># number of observations</span>
    y = np.zeros_like(x)
    <span style="color:#00a">for</span> i, xx <span style="color:#00a">in</span> <span style="color:#0aa">enumerate</span>(x):
        <span style="color:#aaa;font-style:italic"># sum the vectorised indicator function </span>
        y[i] = np.sum(data &lt;= xx)/N 
    <span style="color:#00a">return</span> x, y

x, y = edf(data)
<span style="color:#aaa;font-style:italic"># plot it</span>
plt.plot(x, y, <span style="color:#a50"></span><span style="color:#a50">&#39;</span><span style="color:#a50">k-</span><span style="color:#a50">&#39;</span>)
plt.title(<span style="color:#a50"></span><span style="color:#a50">&#34;</span><span style="color:#a50">Empirical distribution function - $</span><span style="color:#a50">\</span><span style="color:#a50">hat{F}(x)$</span><span style="color:#a50">&#34;</span>)
plt.xlabel(<span style="color:#a50"></span><span style="color:#a50">&#34;</span><span style="color:#a50">$x$</span><span style="color:#a50">&#34;</span>)
plt.ylabel(<span style="color:#a50"></span><span style="color:#a50">&#34;</span><span style="color:#a50">Density</span><span style="color:#a50">&#34;</span>)
plt.plot(data, [<span style="color:#099">0.01</span>]*<span style="color:#0aa">len</span>(data), <span style="color:#a50"></span><span style="color:#a50">&#39;</span><span style="color:#a50">|</span><span style="color:#a50">&#39;</span>, color=<span style="color:#a50"></span><span style="color:#a50">&#39;</span><span style="color:#a50">k</span><span style="color:#a50">&#39;</span>)
sns.despine()
</code></pre></div><p><img src="/img//EDF_6_0.png" alt="png"></p>
<p>$\hat{F}(x)$ looks at least superficially similar to the cumulative density shown in the plot above, increasing rapidly from 0 and then plateaus as the probabilities reach 1. The <code>plt.plot(data, [0.01]*len(data), '|', color='k')</code> here is a <a href="https://stackoverflow.com/questions/17875341/how-to-make-rug-plot-in-matplotlib">nice trick</a> for indicating sample points as we would with a <a href="https://en.wikipedia.org/wiki/Rug_plot">rugplot</a>.</p>
<h2 id="how-good-is-our-distribution-getting-confidence-intervals">How good is our distribution? Getting confidence intervals</h2>
<p>If we want to get some sort of idea of how constrained our EDF is we want to put some uncertainties on it.  Ultimately we want an idea of how far our empirical distribution $\hat{F}(x)$ is from the real unknown CDF $F(x)$.</p>
<p>The <a href="https://en.wikipedia.org/wiki/Dvoretzky%E2%80%93Kiefer%E2%80%93Wolfowitz_inequality">Dvoretzky–Kiefer–Wolfowitz (DKW)</a> inequality provides a method to bound the probability of the agreement between $\hat{F}(x)$ and $F(x)$.  From the DKW inequality, a non-parametric upper and lower confidence interval that bounds $F(x)$ within ($1-\alpha$) is:</p>
<p>$$ \text{Lower}[\hat{F}(x)] = \text{max} [ \hat{F}(x) - \epsilon, \; 0 ] $$</p>
<p>$$  \text{Upper}[\hat{F}(x)] = \text{min} [ \hat{F}(x) + \epsilon, \; 1 ] $$</p>
<p>where $\epsilon$ is:</p>
<p>$$  \epsilon =  \sqrt{ \frac{1}{2n} \text{log}\left(\frac{2}{\alpha}\right)}$$</p>
<p>The direct interpretation of $\epsilon$ is that anywhere along the EDF, the probability that $\hat{F}(x)$ is more than $\epsilon$ away from the true CDF $F(x)$ is:</p>
<p>$$ P(| \hat{F}(x)-{F}(x)| &gt; \epsilon) \leq 2e^{-2n\epsilon^2} $$</p>
<p>Being a <a href="https://en.wikipedia.org/wiki/Concentration_inequality">concentration inequality</a>, the DKW provides an upper bound on the confidence interval (a looser confidence interval). Adding this to the <code>edf</code> function:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#00a">def</span> <span style="color:#0a0">edf</span>(data, alpha=.<span style="color:#099">05</span>, x0=None, x1=None ):
    x0 = data.min() <span style="color:#00a">if</span> x0 <span style="color:#00a">is</span> None <span style="color:#00a">else</span> x0
    x1 = data.max() <span style="color:#00a">if</span> x1 <span style="color:#00a">is</span> None <span style="color:#00a">else</span> x1
    x = np.linspace(x0, x1, <span style="color:#099">100</span>)
    N = data.size
    y = np.zeros_like(x)
    l = np.zeros_like(x)
    u = np.zeros_like(x)
    e = np.sqrt(<span style="color:#099">1.0</span>/(<span style="color:#099">2</span>*N) * np.log(<span style="color:#099">2.</span>/alpha))
    <span style="color:#00a">for</span> i, xx <span style="color:#00a">in</span> <span style="color:#0aa">enumerate</span>(x):
        y[i] = np.sum(data &lt;= xx)/N
        l[i] = np.maximum( y[i] - e, <span style="color:#099">0</span> )
        u[i] = np.minimum( y[i] + e, <span style="color:#099">1</span> )
    <span style="color:#00a">return</span> x, y, l, u
</code></pre></div><p>To calculate a 95% confidence interval ($1-0.05$):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">x, y, l, u = edf(data, alpha=<span style="color:#099">0.05</span>)
plt.fill_between(x, l, u)
plt.plot(x, y, <span style="color:#a50"></span><span style="color:#a50">&#39;</span><span style="color:#a50">k-</span><span style="color:#a50">&#39;</span>)
plt.title(<span style="color:#a50"></span><span style="color:#a50">&#34;</span><span style="color:#a50">Empirical distribution function - $</span><span style="color:#a50">\</span><span style="color:#a50">hat{F}(x)$</span><span style="color:#a50">&#34;</span>)
plt.xlabel(<span style="color:#a50"></span><span style="color:#a50">&#34;</span><span style="color:#a50">$x$</span><span style="color:#a50">&#34;</span>)
plt.ylabel(<span style="color:#a50"></span><span style="color:#a50">&#34;</span><span style="color:#a50">Density</span><span style="color:#a50">&#34;</span>)
plt.plot(data, [<span style="color:#099">0.01</span>]*<span style="color:#0aa">len</span>(data), <span style="color:#a50"></span><span style="color:#a50">&#39;</span><span style="color:#a50">|</span><span style="color:#a50">&#39;</span>, color=<span style="color:#a50"></span><span style="color:#a50">&#39;</span><span style="color:#a50">k</span><span style="color:#a50">&#39;</span>)
sns.despine()
</code></pre></div><p><img src="/img//EDF_9_0.png" alt="png"></p>
<p>Which shows that our estimated EDF is not too shabby. The confidence intervals really shine however when we consider the impact of the number of samples we have on the EDF:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">fig, ax = plt.subplots(<span style="color:#099">2</span>,<span style="color:#099">2</span>, figsize=(<span style="color:#099">8</span>,<span style="color:#099">5</span>),
                             sharex=True, sharey=True, 
                             tight_layout=True)
ax = ax.flatten()
sns.despine()
<span style="color:#aaa;font-style:italic"># make a sample space</span>
xcdf = np.linspace(<span style="color:#099">0</span>, <span style="color:#099">14</span>)
<span style="color:#aaa;font-style:italic"># try some different N values</span>
Nsamps = np.logspace(<span style="color:#099">1</span>,<span style="color:#099">2</span>, <span style="color:#099">4</span>).astype(<span style="color:#0aa">int</span>)
<span style="color:#00a">for</span> k, Ns <span style="color:#00a">in</span> <span style="color:#0aa">enumerate</span>(Nsamps):
    data = scipy.stats.gamma(<span style="color:#099">5</span>,<span style="color:#099">1</span>).rvs(Ns)
    <span style="color:#aaa;font-style:italic"># calculate edf</span>
    x, y, l, u = edf(data, x0=<span style="color:#099">0</span>, x1=<span style="color:#099">14</span>)
    ax[k].fill_between(x, l, u)
    ax[k].plot(x, y, <span style="color:#a50"></span><span style="color:#a50">&#39;</span><span style="color:#a50">k-</span><span style="color:#a50">&#39;</span>)
    ax[k].set_title(f<span style="color:#a50"></span><span style="color:#a50">&#34;</span><span style="color:#a50">Samples: {Ns}</span><span style="color:#a50">&#34;</span>)
    ax[k].set_xlabel(<span style="color:#a50"></span><span style="color:#a50">&#34;</span><span style="color:#a50">$x$</span><span style="color:#a50">&#34;</span>)
    ax[k].set_ylabel(<span style="color:#a50"></span><span style="color:#a50">&#34;</span><span style="color:#a50">Density</span><span style="color:#a50">&#34;</span>)
    ax[k].plot(xcdf, scipy.stats.gamma(<span style="color:#099">5</span>,<span style="color:#099">1</span>).cdf(xcdf), 
                <span style="color:#a50"></span><span style="color:#a50">&#39;</span><span style="color:#a50">k--</span><span style="color:#a50">&#39;</span>, linewidth=<span style="color:#099">1</span>   )
    ax[k].plot(data, [<span style="color:#099">0.01</span>]*<span style="color:#0aa">len</span>(data), <span style="color:#a50"></span><span style="color:#a50">&#39;</span><span style="color:#a50">|</span><span style="color:#a50">&#39;</span>, color=<span style="color:#a50"></span><span style="color:#a50">&#39;</span><span style="color:#a50">k</span><span style="color:#a50">&#39;</span>)
</code></pre></div><p><img src="/img//EDF_11_0.png" alt="png"></p>
<p>So the width of 95% confidence interval shrinks proportionally to $N$, providing us with a clear measure of the trust we can put in our cumulative distribution. We can get an idea of the width of the confidence interval from considering $\epsilon$ as a function of the number of observations $N$:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">N = np.logspace(<span style="color:#099">1</span>, <span style="color:#099">4</span>)
plt.plot(N, np.sqrt( <span style="color:#099">1.0</span>/(<span style="color:#099">2</span>*N) *np.log(<span style="color:#099">2.</span>/.<span style="color:#099">32</span>)), label=<span style="color:#a50"></span><span style="color:#a50">&#39;</span><span style="color:#a50">$1</span><span style="color:#a50">\</span><span style="color:#a50">sigma$</span><span style="color:#a50">&#39;</span>)
plt.plot(N, np.sqrt( <span style="color:#099">1.0</span>/(<span style="color:#099">2</span>*N) *np.log(<span style="color:#099">2.</span>/.<span style="color:#099">05</span>)), label=<span style="color:#a50"></span><span style="color:#a50">&#39;</span><span style="color:#a50">$2</span><span style="color:#a50">\</span><span style="color:#a50">sigma$</span><span style="color:#a50">&#39;</span>)
plt.semilogx()
plt.ylabel(<span style="color:#a50"></span><span style="color:#a50">&#34;</span><span style="color:#a50">$</span><span style="color:#a50">\</span><span style="color:#a50">epsilon$</span><span style="color:#a50">&#34;</span>)
plt.xlabel(<span style="color:#a50"></span><span style="color:#a50">&#34;</span><span style="color:#a50">Number of samples</span><span style="color:#a50">&#34;</span>)
plt.legend(loc=<span style="color:#a50"></span><span style="color:#a50">&#39;</span><span style="color:#a50">best</span><span style="color:#a50">&#39;</span>)
sns.despine()
</code></pre></div><p><img src="/img//EDF_13_0.png" alt="png"></p>
			</div>

			<div class="tags">
				
					
						<ul class="flat">
							
							<li><a href="/tags/statistics">statistics</a></li>
							
							<li><a href="/tags/probability">probability</a></li>
							
							<li><a href="/tags/uncertainty">uncertainty</a></li>
							
							<li><a href="/tags/python">python</a></li>
							
							<li><a href="/tags/cdf">CDF</a></li>
							
						</ul>
					
				
			</div></div>
	</div>
	<script type="text/javascript"
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]} })
</script>


<div class="footer wrapper">
	<nav class="nav">
		<div>2020  <a href="https://github.com/knadh/hugo-ink">Ink</a> theme on <a href="https://gohugo.io">Hugo</a></div>
	</nav>
</div>

<script>feather.replace()</script>
</body>
</html>
