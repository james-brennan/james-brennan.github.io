<!DOCTYPE html>
<html>

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<title> Confidence intervals for LOWESS models in python &middot; James Brennan </title>


<link rel="stylesheet" href="https://james-brennan.github.io/css/slim.css">
<link rel="stylesheet" href="https://james-brennan.github.io/css/highlight.min.css">
<link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700|Source+Code+Pro' rel='stylesheet' type='text/css'>

<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
<link rel="shortcut icon" href="/favicon.ico">


<link href="" rel="alternate" type="application/rss+xml" title="James Brennan" />

</head>

<body>
  <div class="container">
    <div class="header">
  <h1 class="site-title"><a href="https://james-brennan.github.io/">James Brennan</a></h1>
  <p class="site-tagline">A blog about scientific computing, statistics, geoscience and remote sensing. </p>
  <div class="nav">
    <a class="nav-btn" href="#">
      <span class="ci ci-burger"></span>
    </a>
    <ul class="nav-list">
       
	  <li class="spacer">&ac;</li>

      <li><a href="https://github.com/james-brennan">Github</a></li> 
      <li><a href="https://twitter.com/jamesbrennan">Twitter</a></li> 
    </ul>
  </div>
</div>

    <div class="content">
      <div class="post">
        <h2 class="post-title"><a href="https://james-brennan.github.io/posts/lowess_conf/">Confidence intervals for LOWESS models in python</a></h2>
        <div class="post-content">
          <p>LOWESS (or also referred to as LOESS for <em>locally-weighted scatterplot smoothing</em>) is a non-parametric regression method for smoothing data. But how do we get uncertainties on the curve?</p>
<p>The &ldquo;non-parametric&rdquo;-ness of the method refers to the fact that unlike linear or non-linear regression, the model can&rsquo;t be parameterised &ndash; we can&rsquo;t write the model as the sum of several parametric terms such as with linear regression:</p>
<p>$$ y = \beta_0 + \beta_1 x $$</p>
<p>Instead, LOWESS attempts to fit a linear model for each observation based on nearby (or <em>local</em>) observations. This makes the method more versatile than fitting some n-order polynomial to wiggly data but of course, means that we do not have global parameters such as $ \beta_0 $ and $\beta_1$ with which to test hypotheses. To start looking at LOWESS methods, let&rsquo;s create some nonsense data which has some interesting nonmonotonic response and plot it.</p>
<div class="highlight"><pre style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">numpy</span> <span style="color:#007020;font-weight:bold">as</span> <span style="color:#0e84b5;font-weight:bold">np</span>
<span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">matplotlib.pyplot</span> <span style="color:#007020;font-weight:bold">as</span> <span style="color:#0e84b5;font-weight:bold">plt</span>
<span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">seaborn</span> <span style="color:#007020;font-weight:bold">as</span> <span style="color:#0e84b5;font-weight:bold">sns</span>
<span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">matplotlib.pyplot</span> <span style="color:#007020;font-weight:bold">as</span> <span style="color:#0e84b5;font-weight:bold">plt</span>
sns<span style="color:#666">.</span>set_style(<span style="color:#4070a0"></span><span style="color:#4070a0">&#34;</span><span style="color:#4070a0">white</span><span style="color:#4070a0">&#34;</span>)
plt<span style="color:#666">.</span>rc(<span style="color:#4070a0"></span><span style="color:#4070a0">&#34;</span><span style="color:#4070a0">axes.spines</span><span style="color:#4070a0">&#34;</span>, top<span style="color:#666">=</span>False, right<span style="color:#666">=</span>False)
sns<span style="color:#666">.</span>set_context(<span style="color:#4070a0"></span><span style="color:#4070a0">&#34;</span><span style="color:#4070a0">paper</span><span style="color:#4070a0">&#34;</span>)

<span style="color:#60a0b0;font-style:italic"># make some data</span>
x <span style="color:#666">=</span> <span style="color:#40a070">5</span><span style="color:#666">*</span>np<span style="color:#666">.</span>random<span style="color:#666">.</span>random(<span style="color:#40a070">100</span>)
y <span style="color:#666">=</span> np<span style="color:#666">.</span>sin(x) <span style="color:#666">*</span> <span style="color:#40a070">3</span><span style="color:#666">*</span>np<span style="color:#666">.</span>exp(<span style="color:#666">-</span>x) <span style="color:#666">+</span> np<span style="color:#666">.</span>random<span style="color:#666">.</span>normal(<span style="color:#40a070">0</span>, <span style="color:#40a070">0.2</span>, <span style="color:#40a070">100</span>)
plt<span style="color:#666">.</span>plot(x, y, <span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">k.</span><span style="color:#4070a0">&#39;</span>)
</code></pre></div><p><img src="/img/lowess1_3_1.png" alt="png"></p>
<h1 id="statsmodels">statsmodels</h1>
<p>Python package <a href="https://www.statsmodels.org/">statsmodels</a> has an efficient LOWESS smoother built-in which provides the obvious choice for doing a <a href="https://www.statsmodels.org/stable/generated/statsmodels.nonparametric.smoothers_lowess.lowess.html">lowess smoother</a> in python:</p>
<div class="highlight"><pre style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#007020;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">statsmodels.nonparametric.smoothers_lowess</span> <span style="color:#007020;font-weight:bold">import</span> lowess <span style="color:#007020;font-weight:bold">as</span>  sm_lowess
sm_x, sm_y <span style="color:#666">=</span> sm_lowess(y, x,  frac<span style="color:#666">=</span><span style="color:#40a070">1.</span><span style="color:#666">/</span><span style="color:#40a070">5.</span>, 
                           it<span style="color:#666">=</span><span style="color:#40a070">5</span>, return_sorted <span style="color:#666">=</span> True)<span style="color:#666">.</span>T
plt<span style="color:#666">.</span>plot(sm_x, sm_y, color<span style="color:#666">=</span><span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">tomato</span><span style="color:#4070a0">&#39;</span>)
plt<span style="color:#666">.</span>plot(x, y, <span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">k.</span><span style="color:#4070a0">&#39;</span>)
</code></pre></div><p><img src="/img/lowess1_5_1.png" alt="png"></p>
<p>So that looks fairly sensible, <code>sm_lowess</code> has picked out the main structure in the data but not overfitted the curve to any of the noise in the data.</p>
<p>Of course, judging the quality of the fit is difficult because we don&rsquo;t really have an idea of the uncertainty. Ideally, we&rsquo;d have a LOWESS method which provides us a confidence interval on the fit of the model so we can see which individual observations are within the fit and those outside of the fitted curve.</p>
<h1 id="bootstrapping">Bootstrapping</h1>
<p>One obvious way to get a confidence interval around the LOWESS fit is to use bootstrapping to provide an estimate of the spread of the curve. We&rsquo;ll sample from the observations to change the distribution of points for each fit each time:</p>
<div class="highlight"><pre style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">scipy.interpolate</span>

<span style="color:#007020;font-weight:bold">def</span> <span style="color:#06287e">smooth</span>(x, y, xgrid):
    samples <span style="color:#666">=</span> np<span style="color:#666">.</span>random<span style="color:#666">.</span>choice(<span style="color:#007020">len</span>(x), <span style="color:#40a070">50</span>, replace<span style="color:#666">=</span>True)
    y_s <span style="color:#666">=</span> y[samples]
    x_s <span style="color:#666">=</span> x[samples]
    y_sm <span style="color:#666">=</span> sm_lowess(y_s,x_s, frac<span style="color:#666">=</span><span style="color:#40a070">1.</span><span style="color:#666">/</span><span style="color:#40a070">5.</span>, it<span style="color:#666">=</span><span style="color:#40a070">5</span>,
                     return_sorted <span style="color:#666">=</span> False)
    <span style="color:#60a0b0;font-style:italic"># regularly sample it onto the grid</span>
    y_grid <span style="color:#666">=</span> scipy<span style="color:#666">.</span>interpolate<span style="color:#666">.</span>interp1d(x_s, y_sm, 
                                        fill_value<span style="color:#666">=</span><span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">extrapolate</span><span style="color:#4070a0">&#39;</span>)(xgrid)
    <span style="color:#007020;font-weight:bold">return</span> y_grid
</code></pre></div><p>So <code>smooth</code> samples 50% of the observations and fits the LOWESS model. Also because statsmodels doest not provide the solution on an interpolated, and we&rsquo;re randomly sampling each, the solution is interpolated to the same 1d grid each time specified with <code>xgrid</code>.  Let&rsquo;s run <code>smooth</code> 100 times and plot each lowess solution:</p>
<div class="highlight"><pre style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">xgrid <span style="color:#666">=</span> np<span style="color:#666">.</span>linspace(x<span style="color:#666">.</span>min(),x<span style="color:#666">.</span>max())
K <span style="color:#666">=</span> <span style="color:#40a070">100</span>
smooths <span style="color:#666">=</span> np<span style="color:#666">.</span>stack([smooth(x, y, xgrid) <span style="color:#007020;font-weight:bold">for</span> k <span style="color:#007020;font-weight:bold">in</span> <span style="color:#007020">range</span>(K)])<span style="color:#666">.</span>T
plt<span style="color:#666">.</span>plot(xgrid, smooths, color<span style="color:#666">=</span><span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">tomato</span><span style="color:#4070a0">&#39;</span>, alpha<span style="color:#666">=</span><span style="color:#40a070">0.25</span>)
plt<span style="color:#666">.</span>plot(x, y, <span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">k.</span><span style="color:#4070a0">&#39;</span>)
</code></pre></div><p><img src="/img/lowess1_9_2.png" alt="png"></p>
<p>We can then use the individual fits to provide the mean $\mu$ and standard error $\sigma$ of the LOWESS model:</p>
<div class="highlight"><pre style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">mean <span style="color:#666">=</span> np<span style="color:#666">.</span>nanmean(smooths, axis<span style="color:#666">=</span><span style="color:#40a070">1</span>)
stderr <span style="color:#666">=</span> scipy<span style="color:#666">.</span>stats<span style="color:#666">.</span>sem(smooths, axis<span style="color:#666">=</span><span style="color:#40a070">1</span>)
stderr <span style="color:#666">=</span> np<span style="color:#666">.</span>nanstd(smooths, axis<span style="color:#666">=</span><span style="color:#40a070">1</span>, ddof<span style="color:#666">=</span><span style="color:#40a070">0</span>)
<span style="color:#60a0b0;font-style:italic"># plot it</span>
plt<span style="color:#666">.</span>fill_between(xgrid, mean<span style="color:#666">-</span><span style="color:#40a070">1.96</span><span style="color:#666">*</span>stderr,
                     mean<span style="color:#666">+</span><span style="color:#40a070">1.96</span><span style="color:#666">*</span>stderr, alpha<span style="color:#666">=</span><span style="color:#40a070">0.25</span>)
plt<span style="color:#666">.</span>plot(xgrid, mean, color<span style="color:#666">=</span><span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">tomato</span><span style="color:#4070a0">&#39;</span>)
plt<span style="color:#666">.</span>plot(x, y, <span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">k.</span><span style="color:#4070a0">&#39;</span>)
</code></pre></div><p><img src="/img/lowess1_11_1.png" alt="png"></p>
<p>The 95% confidence interval (shaded blue) seems fairly sensible - the uncertainty increases when observations nearby have a large spread (at around x=2) but also at the edges of the plot where the number of observations tends towards zero (at the very edge we only have observations from the left or right to do the smoothing).</p>
<p>Similarly, because bootstrapping provides draws from the posterior of the LOWESS smooth we can create a true confidence interval from any percentiles:</p>
<div class="highlight"><pre style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">mean <span style="color:#666">=</span> np<span style="color:#666">.</span>nanmean(smooths, axis<span style="color:#666">=</span><span style="color:#40a070">1</span>)
c25 <span style="color:#666">=</span> np<span style="color:#666">.</span>nanpercentile(smooths, <span style="color:#40a070">2.5</span>, axis<span style="color:#666">=</span><span style="color:#40a070">1</span>) <span style="color:#60a0b0;font-style:italic">#2.5 percent</span>
c97 <span style="color:#666">=</span> np<span style="color:#666">.</span>nanpercentile(smooths, <span style="color:#40a070">97.5</span>, axis<span style="color:#666">=</span><span style="color:#40a070">1</span>) <span style="color:#60a0b0;font-style:italic"># 97.5 percent</span>
<span style="color:#60a0b0;font-style:italic"># a 95% confidence interval</span>
plt<span style="color:#666">.</span>fill_between(xgrid, mean<span style="color:#666">-</span><span style="color:#40a070">1.96</span><span style="color:#666">*</span>stderr,
                     mean<span style="color:#666">+</span><span style="color:#40a070">1.96</span><span style="color:#666">*</span>stderr, alpha<span style="color:#666">=</span><span style="color:#40a070">0.25</span>)
plt<span style="color:#666">.</span>plot(xgrid, mean, color<span style="color:#666">=</span><span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">tomato</span><span style="color:#4070a0">&#39;</span>)
plt<span style="color:#666">.</span>plot(x, y, <span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">k.</span><span style="color:#4070a0">&#39;</span>)
</code></pre></div><p><img src="/img/lowess1_13_1.png" alt="png"></p>
<h1 id="writing-a-custom-lowess-smoother">Writing a custom LOWESS smoother</h1>
<p>Notice the similarity in the $\mu + 1.96\sigma$ confidence interval and the percentile-based 95% confidence interval. Actually, under certain assumptions about the errors in the data, these should be the same. This is because LOWESS smoothers essentially fit a unique linear regression for every data point by including nearby data points to estimate the slope and intercept. The correlation in nearby data points helps ensure that we get a smooth curve fit.  We can understand this a bit more clearly by estimating the curve locally for a couple of observations with linear regression:</p>
<div class="highlight"><pre style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#007020;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">scipy.stats</span>
order <span style="color:#666">=</span> np<span style="color:#666">.</span>argsort(x) <span style="color:#60a0b0;font-style:italic"># order the points</span>
k <span style="color:#666">=</span> <span style="color:#40a070">50</span>
xx <span style="color:#666">=</span> x[order][k<span style="color:#666">-</span><span style="color:#40a070">5</span>:k<span style="color:#666">+</span><span style="color:#40a070">5</span>]
yy <span style="color:#666">=</span> y[order][k<span style="color:#666">-</span><span style="color:#40a070">5</span>:k<span style="color:#666">+</span><span style="color:#40a070">5</span>]
plt<span style="color:#666">.</span>plot(x, y, <span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">k.</span><span style="color:#4070a0">&#39;</span>)
plt<span style="color:#666">.</span>plot(xx, yy, <span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">rs</span><span style="color:#4070a0">&#39;</span>)
res <span style="color:#666">=</span> scipy<span style="color:#666">.</span>stats<span style="color:#666">.</span>linregress(xx, yy)
plt<span style="color:#666">.</span>plot(xx, res<span style="color:#666">.</span>intercept <span style="color:#666">+</span> res<span style="color:#666">.</span>slope <span style="color:#666">*</span>xx, <span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">r-</span><span style="color:#4070a0">&#39;</span>)
k <span style="color:#666">=</span> <span style="color:#40a070">60</span>
xx <span style="color:#666">=</span> x[order][k<span style="color:#666">-</span><span style="color:#40a070">5</span>:k<span style="color:#666">+</span><span style="color:#40a070">5</span>]
yy <span style="color:#666">=</span> y[order][k<span style="color:#666">-</span><span style="color:#40a070">5</span>:k<span style="color:#666">+</span><span style="color:#40a070">5</span>]
plt<span style="color:#666">.</span>plot(x, y, <span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">k.</span><span style="color:#4070a0">&#39;</span>)
plt<span style="color:#666">.</span>plot(xx, yy, <span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">gs</span><span style="color:#4070a0">&#39;</span>)
res <span style="color:#666">=</span> scipy<span style="color:#666">.</span>stats<span style="color:#666">.</span>linregress(xx, yy)
plt<span style="color:#666">.</span>plot(xx, res<span style="color:#666">.</span>intercept <span style="color:#666">+</span> res<span style="color:#666">.</span>slope <span style="color:#666">*</span>xx, <span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">g-</span><span style="color:#4070a0">&#39;</span>)
</code></pre></div><p><img src="/img/lowess1_16_1.png" alt="png"></p>
<p>Extending this principle we can get something that looks a bit like the curve from earlier:</p>
<div class="highlight"><pre style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">order <span style="color:#666">=</span> np<span style="color:#666">.</span>argsort(x)
<span style="color:#007020;font-weight:bold">for</span> k <span style="color:#007020;font-weight:bold">in</span> <span style="color:#007020">range</span>(<span style="color:#40a070">5</span>, <span style="color:#007020">len</span>(y)<span style="color:#666">-</span><span style="color:#40a070">5</span> ):
    xx <span style="color:#666">=</span> x[order][k<span style="color:#666">-</span><span style="color:#40a070">5</span>:k<span style="color:#666">+</span><span style="color:#40a070">5</span>]
    yy <span style="color:#666">=</span> y[order][k<span style="color:#666">-</span><span style="color:#40a070">5</span>:k<span style="color:#666">+</span><span style="color:#40a070">5</span>]
    plt<span style="color:#666">.</span>plot(xx, yy, <span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">.</span><span style="color:#4070a0">&#39;</span>)
    res <span style="color:#666">=</span> scipy<span style="color:#666">.</span>stats<span style="color:#666">.</span>linregress(xx, yy)
    plt<span style="color:#666">.</span>plot(xx, res<span style="color:#666">.</span>intercept <span style="color:#666">+</span> res<span style="color:#666">.</span>slope <span style="color:#666">*</span>xx, <span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">-</span><span style="color:#4070a0">&#39;</span>)
</code></pre></div><p><img src="/img/lowess1_18_0.png" alt="png"></p>
<p>Instead of just selecting the 5 nearest data points and fitting a simple linear regression, LOWESS weights the points based on the proximity of neighbouring points. This is what specifies LOWESS smoothing as just another <a href="https://en.wikipedia.org/wiki/Kernel_smoother">kernel smoother</a> method in which a function $F$ is used to weight observations based on proximity. For LOWESS, the most commonly used function is the <a href="https://en.wikipedia.org/wiki/Kernel_(statistics)#Kernel_functions_in_common_use">tricube</a>:</p>
<div class="highlight"><pre style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">tricube <span style="color:#666">=</span> <span style="color:#007020;font-weight:bold">lambda</span> d : (<span style="color:#40a070">1</span> <span style="color:#666">-</span> np<span style="color:#666">.</span>abs(d)<span style="color:#666">*</span><span style="color:#666">*</span><span style="color:#40a070">3</span>)<span style="color:#666">*</span><span style="color:#666">*</span><span style="color:#40a070">3</span>
d <span style="color:#666">=</span> np<span style="color:#666">.</span>linspace(<span style="color:#40a070">0</span>,<span style="color:#40a070">1</span>)
plt<span style="color:#666">.</span>plot(d, tricube(d))
plt<span style="color:#666">.</span>xlabel(<span style="color:#4070a0"></span><span style="color:#4070a0">&#34;</span><span style="color:#4070a0">$d$</span><span style="color:#4070a0">&#34;</span>)
plt<span style="color:#666">.</span>ylabel(<span style="color:#4070a0"></span><span style="color:#4070a0">&#34;</span><span style="color:#4070a0">(1 - np.abs(d)**3)**3</span><span style="color:#4070a0">&#34;</span>)
</code></pre></div><p><img src="/img/lowess1_20_1.png" alt="png"></p>
<h2 id="fitting-the-lowess-curve">Fitting the LOWESS curve</h2>
<p>It&rsquo;s helpful (well necessary) to delve into the LOWESS mathematics at this point. The observations (a collection of x and y points we&rsquo;ll say) are composed of $x$ and $y$ vectors:</p>
<p>$$ x = [x_1 \cdots x_N] $$</p>
<p>$$ y =  [y_1 \cdots y_N] $$</p>
<p>For observation $x_i$, the LOWESS fitting starts with calculating the distance between $x_i$ and each other observation $x_j$ provides:</p>
<p>$$ d_{ij} =  [y_{i0} \cdots y_{iN}] $$</p>
<p>With these distances, the tricube weighting function is used to up-weight observations which are more local:</p>
<p>$$ w_i = (1 - |d_{ij}|^3)^3  $$</p>
<p>With $w$ we solve a standard weighted least squares system:</p>
<p>$$ \hat{\beta} = \left(\mathbf{X}^T \mathbf{W}  \mathbf{X} \right)^{-1} \mathbf{X}^T \mathbf{W} y $$</p>
<p>where $\mathbf{X}$ is specified as a first order model:</p>
<p>$$
\begin{equation}
\mathbf{X} =
\begin{bmatrix}
1 &amp;  x_1 \\\<br>
\vdots &amp; \vdots  \\\<br>
1 &amp;  x_N
\end{bmatrix}
\end{equation}
$$</p>
<p>which is a linear model with an intercept (1 values) and the slope. It&rsquo;s possible to use higher order polynomials of course such as $x^2$ but we&rsquo;ll stick with the simplest case here.  $\mathbf{W}$ is just a diagonal matrix formed from the weights $w_i$:</p>
<p>$$ \mathbf{W} = \begin{bmatrix}
w_1 &amp;  &amp; \\\<br>
&amp; \ddots &amp; \\\<br>
&amp;  &amp; w_N
\end{bmatrix} $$</p>
<p>Solving the system provides $\hat{\beta}$ - the intercept $\beta_0$ and slope $\beta_1$. The LOWESS smoothed observation $\hat{y}_{sm}$ is fitted/predicted from the row of the system corresponding to $i$:</p>
<p>$$ \hat{y}_{sm} = \mathbf{X}_i \hat{\beta}  $$</p>
<p>So algorithmically we loop over each observation $i$ update the $\mathbf{W}$ with the new weights and resolve the least squares system above to update $\hat{\beta}$ and then predict each $\hat{y}_{sm}$.</p>
<p>In code this relatively simple to implement for one observation $i$:</p>
<div class="highlight"><pre style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">dist <span style="color:#666">=</span> np<span style="color:#666">.</span>abs((x[i]<span style="color:#666">-</span>x))
w <span style="color:#666">=</span> tricube(dist)
<span style="color:#60a0b0;font-style:italic"># form linear system + apply the weights</span>
A <span style="color:#666">=</span> np<span style="color:#666">.</span>stack([w, x<span style="color:#666">*</span>w])<span style="color:#666">.</span>T
b <span style="color:#666">=</span> w <span style="color:#666">*</span> y
ATA <span style="color:#666">=</span> A<span style="color:#666">.</span>T<span style="color:#666">.</span>dot(A)
ATb <span style="color:#666">=</span> A<span style="color:#666">.</span>T<span style="color:#666">.</span>dot(b)
<span style="color:#60a0b0;font-style:italic"># solve the system</span>
sol <span style="color:#666">=</span> np<span style="color:#666">.</span>linalg<span style="color:#666">.</span>solve(ATA, ATb)
<span style="color:#60a0b0;font-style:italic"># predict for the observation only</span>
yest <span style="color:#666">=</span> A[i]<span style="color:#666">.</span>dot(sol)
</code></pre></div><h2 id="confidence-intervals-for-lowess">Confidence intervals for LOWESS</h2>
<p>Because the LOWESS smoother for any individual prediction is essentially weighted linear least squares, the propagation of uncertainty principles are well understood. With any linear model (with normal errors in the observations $y$) the uncertainty in the parameters $var(\hat{\beta})$ are provided from the posterior covariance matrix:</p>
<p>$$\sigma^2\left(\mathbf{X}^T  \mathbf{X}\right)^{-1}$$</p>
<p>The diagonal of this matrix provides $var(\hat{\beta})$:</p>
<p>$$ var(\hat{\beta}) = \text{diag} \left[ \sigma^2 \left(\mathbf{X}^T  \mathbf{X}\right)^{-1} \right] $$</p>
<p>$\sigma^2$ is <em>important</em> - it is our estimate of the uncertainty in the initial observations. For this approach to work, we are assuming that the observations have an error model of the form:</p>
<p>$$ y = y_\text{true} + \mathcal{N}(0, \sigma^2) $$</p>
<p>The observational error $\sigma^2$ provides an indication of the spread in the observations away from our model of the observations:</p>
<p>$$ \hat{y}_{sm} = f(x) + \epsilon $$</p>
<p>where $f(x)$ is the LOWESS smoothing model and $\epsilon$ is $\mathcal{N}(0, \sigma^2)$ &ndash; the error arising in the prediction because of the observations. In some cases, we know what $\sigma^2$ is, but often we don&rsquo;t. One option to estimate $\sigma^2$ is to assume that our model is perfect and the residuals between the model and observations $(\hat{y}_{sm}-y)$ provides a good estimate. The root mean square error (RMSE) provides this estimate of $\sigma$:</p>
<p>$$ \sigma = \sqrt{\frac{\sum_i^{N}( \hat{y}_{sm} - y_i  )^2}{N}} $$</p>
<p>With an estimate of $\sigma$ we can then estimate $var(\hat{\beta})$ correctly and provide a confidence interval based on the assumption that the uncertainty in the parameters is normally distributed. For example a 95 confidence interval on the slope parameter $\hat{\beta_1}$ is:</p>
<p>$$ \text{CI}_{0.95} = \hat{\beta_1} \pm 1.96 \sqrt{var(\hat{\beta_1}) } $$</p>
<p>So we&rsquo;ve now got a way to get the confidence interval in parameters $\hat{\beta}$ from the variance $var(\hat{\beta})$ but we really want the confidence interval for the fitted curve $\hat{y}_{sm}$.</p>
<p>To get this remember that $\hat{y}_{sm}$ is provided by:</p>
<p>$$ \hat{y}_{sm} = \mathbf{X}_i \hat{\beta} $$</p>
<p>To get the variance or uncertainty in the prediction $var(\hat{y}_{sm})$ we apply standard least squares uncertainty propagation:</p>
<p>$$ var(\hat{y}_{sm}) = var(\mathbf{X}_i \hat{\beta}) = \mathbf{X}_i^T \sigma^2 \left(\mathbf{X}^T  \mathbf{X}\right)^{-1} \mathbf{X}_i$$</p>
<p>In code this relatively simple to implement also:</p>
<div class="highlight"><pre style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">var_y_sm <span style="color:#666">=</span> A[i]<span style="color:#666">.</span>dot(sigma<span style="color:#666">*</span><span style="color:#666">*</span><span style="color:#40a070">2</span> <span style="color:#666">*</span> np<span style="color:#666">.</span>linalg<span style="color:#666">.</span>inv(ATA))<span style="color:#666">.</span>dot(A[i])
</code></pre></div><p>Wrapping it all up into a <code>lowess</code> function that loops over each observation and fits the curve and uncertainties:</p>
<div class="highlight"><pre style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#007020;font-weight:bold">def</span> <span style="color:#06287e">lowess</span>(x, y, f<span style="color:#666">=</span><span style="color:#40a070">1.</span><span style="color:#666">/</span><span style="color:#40a070">3.</span>):
    <span style="color:#4070a0"></span><span style="color:#4070a0">&#34;&#34;&#34;</span><span style="color:#4070a0">
</span><span style="color:#4070a0"></span><span style="color:#4070a0">    Basic LOWESS smoother with uncertainty. </span><span style="color:#4070a0">
</span><span style="color:#4070a0"></span><span style="color:#4070a0">    Note:</span><span style="color:#4070a0">
</span><span style="color:#4070a0"></span><span style="color:#4070a0">        - Not robust (so no iteration) and</span><span style="color:#4070a0">
</span><span style="color:#4070a0"></span><span style="color:#4070a0">             only normally distributed errors. </span><span style="color:#4070a0">
</span><span style="color:#4070a0"></span><span style="color:#4070a0">        - No higher order polynomials d=1 </span><span style="color:#4070a0">
</span><span style="color:#4070a0"></span><span style="color:#4070a0">            so linear smoother.</span><span style="color:#4070a0">
</span><span style="color:#4070a0"></span><span style="color:#4070a0">    </span><span style="color:#4070a0">&#34;&#34;&#34;</span>
    <span style="color:#60a0b0;font-style:italic"># get some paras</span>
    xwidth <span style="color:#666">=</span> f<span style="color:#666">*</span>(x<span style="color:#666">.</span>max()<span style="color:#666">-</span>x<span style="color:#666">.</span>min()) <span style="color:#60a0b0;font-style:italic"># effective width after reduction factor</span>
    N <span style="color:#666">=</span> <span style="color:#007020">len</span>(x) <span style="color:#60a0b0;font-style:italic"># number of obs</span>
    <span style="color:#60a0b0;font-style:italic"># Don&#39;t assume the data is sorted</span>
    order <span style="color:#666">=</span> np<span style="color:#666">.</span>argsort(x)
    <span style="color:#60a0b0;font-style:italic"># storage</span>
    y_sm <span style="color:#666">=</span> np<span style="color:#666">.</span>zeros_like(y)
    y_stderr <span style="color:#666">=</span> np<span style="color:#666">.</span>zeros_like(y)
    <span style="color:#60a0b0;font-style:italic"># define the weigthing function -- clipping too!</span>
    tricube <span style="color:#666">=</span> <span style="color:#007020;font-weight:bold">lambda</span> d : np<span style="color:#666">.</span>clip((<span style="color:#40a070">1</span><span style="color:#666">-</span> np<span style="color:#666">.</span>abs(d)<span style="color:#666">*</span><span style="color:#666">*</span><span style="color:#40a070">3</span>)<span style="color:#666">*</span><span style="color:#666">*</span><span style="color:#40a070">3</span>, <span style="color:#40a070">0</span>, <span style="color:#40a070">1</span>)
    <span style="color:#60a0b0;font-style:italic"># run the regression for each observation i</span>
    <span style="color:#007020;font-weight:bold">for</span> i <span style="color:#007020;font-weight:bold">in</span> <span style="color:#007020">range</span>(N):
        dist <span style="color:#666">=</span> np<span style="color:#666">.</span>abs((x[order][i]<span style="color:#666">-</span>x[order]))<span style="color:#666">/</span>xwidth
        w <span style="color:#666">=</span> tricube(dist)
        <span style="color:#60a0b0;font-style:italic"># form linear system with the weights</span>
        A <span style="color:#666">=</span> np<span style="color:#666">.</span>stack([w, x[order]<span style="color:#666">*</span>w])<span style="color:#666">.</span>T
        b <span style="color:#666">=</span> w <span style="color:#666">*</span> y[order]
        ATA <span style="color:#666">=</span> A<span style="color:#666">.</span>T<span style="color:#666">.</span>dot(A)
        ATb <span style="color:#666">=</span> A<span style="color:#666">.</span>T<span style="color:#666">.</span>dot(b)
        <span style="color:#60a0b0;font-style:italic"># solve the syste</span>
        sol <span style="color:#666">=</span> np<span style="color:#666">.</span>linalg<span style="color:#666">.</span>solve(ATA, ATb)
        <span style="color:#60a0b0;font-style:italic"># predict for the observation only</span>
        yest <span style="color:#666">=</span> A[i]<span style="color:#666">.</span>dot(sol)<span style="color:#60a0b0;font-style:italic"># equiv of A.dot(yest) just for k</span>
        place <span style="color:#666">=</span> order[i]
        y_sm[place]<span style="color:#666">=</span>yest 
        sigma2 <span style="color:#666">=</span> (np<span style="color:#666">.</span>sum((A<span style="color:#666">.</span>dot(sol) <span style="color:#666">-</span>y [order])<span style="color:#666">*</span><span style="color:#666">*</span><span style="color:#40a070">2</span>)<span style="color:#666">/</span>N )
        <span style="color:#60a0b0;font-style:italic"># Calculate the standard error</span>
        y_stderr[place] <span style="color:#666">=</span> np<span style="color:#666">.</span>sqrt(sigma2 <span style="color:#666">*</span> 
                                A[i]<span style="color:#666">.</span>dot(np<span style="color:#666">.</span>linalg<span style="color:#666">.</span>inv(ATA)
                                                    )<span style="color:#666">.</span>dot(A[i]))
    <span style="color:#007020;font-weight:bold">return</span> y_sm, y_stderr

<span style="color:#60a0b0;font-style:italic">#run it</span>
y_sm, y_std <span style="color:#666">=</span> lowess(x, y, f<span style="color:#666">=</span><span style="color:#40a070">1.</span><span style="color:#666">/</span><span style="color:#40a070">5.</span>)
<span style="color:#60a0b0;font-style:italic"># plot it</span>
plt<span style="color:#666">.</span>plot(x[order], y_sm[order], color<span style="color:#666">=</span><span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">tomato</span><span style="color:#4070a0">&#39;</span>, label<span style="color:#666">=</span><span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">LOWESS</span><span style="color:#4070a0">&#39;</span>)
plt<span style="color:#666">.</span>fill_between(x[order], y_sm[order] <span style="color:#666">-</span> <span style="color:#40a070">1.96</span><span style="color:#666">*</span>y_std[order],
                 y_sm[order] <span style="color:#666">+</span> <span style="color:#40a070">1.96</span><span style="color:#666">*</span>y_std[order], alpha<span style="color:#666">=</span><span style="color:#40a070">0.3</span>, label<span style="color:#666">=</span><span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">LOWESS uncertainty</span><span style="color:#4070a0">&#39;</span>)
plt<span style="color:#666">.</span>plot(x, y, <span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">k.</span><span style="color:#4070a0">&#39;</span>, label<span style="color:#666">=</span><span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">Observations</span><span style="color:#4070a0">&#39;</span>)
plt<span style="color:#666">.</span>legend(loc<span style="color:#666">=</span><span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">best</span><span style="color:#4070a0">&#39;</span>)
<span style="color:#60a0b0;font-style:italic">#run it</span>
y_sm, y_std <span style="color:#666">=</span> lowess(x, y, f<span style="color:#666">=</span><span style="color:#40a070">1.</span><span style="color:#666">/</span><span style="color:#40a070">5.</span>)
<span style="color:#60a0b0;font-style:italic"># plot it</span>
plt<span style="color:#666">.</span>plot(x[order], y_sm[order], color<span style="color:#666">=</span><span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">tomato</span><span style="color:#4070a0">&#39;</span>, label<span style="color:#666">=</span><span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">LOWESS</span><span style="color:#4070a0">&#39;</span>)
plt<span style="color:#666">.</span>fill_between(x[order], y_sm[order] <span style="color:#666">-</span> y_std[order],
                 y_sm[order] <span style="color:#666">+</span> y_std[order], alpha<span style="color:#666">=</span><span style="color:#40a070">0.3</span>, label<span style="color:#666">=</span><span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">LOWESS uncertainty</span><span style="color:#4070a0">&#39;</span>)
plt<span style="color:#666">.</span>plot(x, y, <span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">k.</span><span style="color:#4070a0">&#39;</span>, label<span style="color:#666">=</span><span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">Observations</span><span style="color:#4070a0">&#39;</span>)
plt<span style="color:#666">.</span>legend(loc<span style="color:#666">=</span><span style="color:#4070a0"></span><span style="color:#4070a0">&#39;</span><span style="color:#4070a0">best</span><span style="color:#4070a0">&#39;</span>)
</code></pre></div><p><img src="/img/lowess1_22_1.png" alt="png"></p>
        </div>
      </div>
    </div>
    
<script type="text/javascript"
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]} })
</script>



<div class="footer">
  
  <p>Copyright James Brennan 2019. Powered by Hugo.</p>
  
</div>

  </div>
  <script src="https://james-brennan.github.io/js/slim.js"></script>
  <script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-56616557-2', 'auto');
ga('send', 'pageview');

</script>


</body>

</html>